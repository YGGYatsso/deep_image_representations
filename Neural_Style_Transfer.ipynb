{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM3mxUVZK_MD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import cv2 as cv\n",
        "\n",
        "#from google.colab.patches import cv2_imshow\n",
        "from collections import OrderedDict\n",
        "import cv2 as cv\n",
        "from PIL import Image\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "#importing all libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 512\n",
        "prep = transforms.Compose([transforms.Resize((img_size,img_size)),\n",
        "                           transforms.ToTensor(),\n",
        "                          # transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
        "                           transforms.Normalize(mean=[0.485, 0.456, 0.406], #subtract imagenet mean\n",
        "                                                std=[1,1,1]),\n",
        "                           transforms.Lambda(lambda x: x.mul_(255)),\n",
        "                          ])\n",
        "postpa = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n",
        "                           transforms.Normalize(mean=[-0.485, -0.456, -0.406], #add imagenet mean\n",
        "                                                std=[1,1,1]),\n",
        "                     #      transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
        "                           ])\n",
        "postpb = transforms.Compose([transforms.ToPILImage()])\n",
        "def postp(tensor): # to clip results in the range [0,1]\n",
        "    t = postpa(tensor)\n",
        "    t[t>1] = 1\n",
        "    t[t<0] = 0\n",
        "    img = postpb(t)\n",
        "    return img"
      ],
      "metadata": {
        "id": "_1ZoCckYLID-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function and transforms:\n",
        "def Gram_mat(data):\n",
        "  a,channel,height,width=data.shape\n",
        "  new_data=data.view(a*channel,height*width)\n",
        "  gram_val=torch.mm(new_data,new_data.t())/(height*width)\n",
        "\n",
        "  return gram_val\n",
        "\n",
        "def content_loss(current,actual):\n",
        "  #print(current.shape,actual.shape)\n",
        "  a,channel,height,width=current.shape\n",
        "  get=torch.norm(current-actual)/(a*channel*height*width) #normalised the values\n",
        "  #print(current[:,0,:5,:5])\n",
        "  #print(actual[:,0,:5,:5])\n",
        "\n",
        "  return get #by default forbenius norm or l2-norm\n",
        "\n",
        "def style_loss(style,gen):\n",
        "  a,channels,height,width=style.shape\n",
        "  style_gram=Gram_mat(style)\n",
        "  gen_gram=Gram_mat(gen)\n",
        "  out=torch.norm(style_gram-gen_gram)/(channels**2)\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "QaSxEm2SLNnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleNet(nn.Module):\n",
        "   def __init__(self):\n",
        "    super().__init__()\n",
        "    #PRETRAINED MODEL\n",
        "    self.pretrained = models.vgg19(weights=\"DEFAULT\")\n",
        "    for param in self.pretrained.parameters():\n",
        "      param.requires_grad = False\n",
        "    #all available hooks\n",
        "    self.fhooks = []\n",
        "    # initialising hooks\n",
        "    self.fhooks.append(self.pretrained._modules['features'][0].register_forward_hook(self.hookfn))\n",
        "    self.fhooks.append(self.pretrained._modules['features'][2].register_forward_hook(self.hookfn))\n",
        "    self.fhooks.append(self.pretrained._modules['features'][5].register_forward_hook(self.hookfn))\n",
        "    self.fhooks.append(self.pretrained._modules['features'][7].register_forward_hook(self.hookfn)) #content loss reference\n",
        "    self.fhooks.append(self.pretrained._modules['features'][10].register_forward_hook(self.hookfn))\n",
        "\n",
        "    #store hooks output\n",
        "    self.selected_out = OrderedDict()\n",
        "    self.count=0\n",
        "    self.limit=5 # layers limit ,i.e for how many layer i want the output\n",
        "\n",
        "   def forward(self,x):\n",
        "    y=self.pretrained(x)\n",
        "\n",
        "    return y,self.selected_out\n",
        "\n",
        "   def hookfn(self,layername,inp,out):\n",
        "    self.selected_out[self.count]=out\n",
        "    #print(out[:,0,:5,:5])\n",
        "    #print(self.count,str(layername)) ,one can try to see in which order these are getting printed\n",
        "    self.count+=1\n",
        "    if self.limit==self.count:\n",
        "      self.count=0"
      ],
      "metadata": {
        "id": "ZQwH1zzwLRpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize(model,device,lr=1e-1,iterations=2000,step_size=40):\n",
        "  img_dirs = ['/kaggle/input/styles/style_images/', '/kaggle/input/cheecks/']\n",
        "  img_names = ['vg_houses.jpg', 'Tuebingen_Neckarfront.jpg']\n",
        "  imgs = [Image.open(img_dirs[i] + name) for i,name in enumerate(img_names)]\n",
        "  imgs_torch = [prep(img) for img in imgs]  # here rather than using opencv ,we will use Image and apply transforms to it .\n",
        "  if torch.cuda.is_available():\n",
        "    imgs_torch = [img.unsqueeze(0).cuda() for img in imgs_torch]\n",
        "  else:\n",
        "    imgs_torch = [img.unsqueeze(0) for img in imgs_torch]  # adding an extra dimension and imporoting them to cuda\n",
        "\n",
        "  style_img, content_img = imgs_torch\n",
        "  x=content_img.clone() #already on cuda\n",
        "  x.requires_grad=True  #setitup for gradient optimization .\n",
        "\n",
        "  #x=torch.randn((1,3,256,256)) # since we want to optimize input not model\n",
        "  #print(x.is_leaf)\n",
        "  model=model.to(device)\n",
        "  #print(x.is_leaf)\n",
        "\n",
        "  lr=lr\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  #model.requires_grad(False)\n",
        "\n",
        "  _,style_embed_out=model(style_img) # all for style\n",
        "  style_embed=[style_embed_out[i] for i in range(5)]\n",
        "\n",
        "  _,content_embed_out=model(content_img) #4th is for content\n",
        "  content_embed=content_embed_out[3]\n",
        "\n",
        "  embed=style_embed_out[0]\n",
        "\n",
        "  total_loss=0.0\n",
        "  iterations=iterations\n",
        "  alpha=1\n",
        "  beta=1e3\n",
        "\n",
        "  optimizer=torch.optim.Adam([x],lr=lr)\n",
        "  #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = step_size, gamma = gamma)\n",
        "\n",
        "  #training_loop needs to be written here.\n",
        "  #lets go with adam optimizer\n",
        "\n",
        "  for i in range(iterations):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    _,gen_embed_out=model(x)\n",
        "    #print(gen_embed_out[3]==content_embed_out[3])\n",
        "    c_loss=content_loss(gen_embed_out[3],content_embed)\n",
        "    s_loss=[0.2*(style_loss(gen_embed_out[i],style_embed[i]))  for i in range(5)]\n",
        "\n",
        "    styleloss=0.0\n",
        "    for loss in s_loss:\n",
        "      styleloss+=loss\n",
        "\n",
        "    total_loss=c_loss*alpha+styleloss*beta\n",
        "    #print(c_loss,styleloss,s_loss)\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if i%100 == 99:\n",
        "      print(total_loss)\n",
        "  out_img =postp(x.data[0].cpu().squeeze())\n",
        "  plt.figure(figsize=(7,7))\n",
        "  plt.imshow(out_img)\n",
        "    #recreate_image(content_img)\n",
        "    #recreate_image(style_img)"
      ],
      "metadata": {
        "id": "KRdx6igULUyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=StyleNet()\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pPq2mZ0OLeP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "ju38hHJZLe2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=optimize(model,device)"
      ],
      "metadata": {
        "id": "Zb3grKFQLhTf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}