{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mt45AvyEbRAU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as tf\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "from collections import OrderedDict\n",
        "import cv2 as cv\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qgm0d8Hqrreh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "Vi2Khf7jqVV2"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "   def __init__(self):\n",
        "    super().__init__()\n",
        "    #PRETRAINED MODEL\n",
        "    self.pretrained = models.vgg19(weights=\"DEFAULT\")\n",
        "    for param in self.pretrained.parameters():\n",
        "      param.requires_grad = False\n",
        "    #all available hooks\n",
        "    self.fhooks = []\n",
        "    # initialising hooks\n",
        "    self.fhooks.append(self.pretrained._modules['classifier'][1].register_forward_hook(self.hookfn))\n",
        "    ##self.fhooks.append(self.pretrained._modules['classifier'][1].register_forward_hook(self.hookfn))\n",
        "    #store hooks output\n",
        "    self.selected_out = OrderedDict()\n",
        "    self.count=0\n",
        "    self.limit=1 # layers limit ,i.e for how many layer i want the output\n",
        "\n",
        "   def forward(self,x):\n",
        "    y=self.pretrained(x)\n",
        "\n",
        "    return y,self.selected_out\n",
        "\n",
        "   def hookfn(self,layername,inp,out):\n",
        "    self.selected_out[self.count]=out\n",
        "    #print(self.count,str(layername)) ,one can try to see in which order these are getting printed\n",
        "    self.count+=1\n",
        "    if self.limit==self.count:\n",
        "      self.count=0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "2xVX3tueyhZ0"
      },
      "outputs": [],
      "source": [
        "def l2_loss(current,actual):\n",
        "  return torch.norm(current-actual)/(torch.norm(actual)); #by default forbenius norm or l2-norm\n",
        "\n",
        "def variation_loss(current):\n",
        "  #variation within assumed image.\n",
        "  dx = torch.sum(torch.pow((current[:,:,:,1:] - current[:,:,:,:-1]), 2))\n",
        "  dy = torch.sum(torch.pow((current[:,:,1:,:] - current[:,:,:-1,:]), 2))\n",
        "  #print(dx.shape)\n",
        "  #print(dy.shape)\n",
        "  loss = dx+dy\n",
        "  return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "oBwCRVfLUyBy"
      },
      "outputs": [],
      "source": [
        "transform = tf.Compose(\n",
        "    [tf.ToTensor(),\n",
        "    tf.Normalize(mean=[0.485, 0.456, 0.406],  #normalize image with the mean and standard deviation used for training vgg19,bgr mode\n",
        "                  std=[0.229, 0.224, 0.225]),\n",
        "    tf.CenterCrop(size=(256,256))]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "CnaBhtFoq9Ef"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "HUtBtjEde3vB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def recreate_image(img):\n",
        "    '''\n",
        "    normalise the image again to be able to display\n",
        "    '''\n",
        "    reverse_mean = [-0.485, -0.456, -0.406] #rgb\n",
        "    reverse_std = [1/0.229, 1/0.224, 1/0.225] #rgb\n",
        "    img=img.cpu()\n",
        "    recreated_im = copy.copy(img.data.numpy()[0])\n",
        "    for c in range(3):\n",
        "        recreated_im[c] /= reverse_std[c]\n",
        "        recreated_im[c] -= reverse_mean[c]\n",
        "    recreated_im[recreated_im > 1] = 1 # clipping ...\n",
        "    recreated_im[recreated_im < 0] = 0 #clipping\n",
        "    recreated_im = np.round(recreated_im * 255)\n",
        "\n",
        "    recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n",
        "\n",
        "    #con_image=np.uint8(image*255.0)\n",
        "    con_image=cv.cvtColor(recreated_im, cv.COLOR_RGB2BGR) # rgb->bgr , since cv2_imshow assumes image is in bgr format\n",
        "    cv2_imshow(con_image)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "a4yMWVvOi_J8"
      },
      "outputs": [],
      "source": [
        "#not creating a function for the current instance .\n",
        " # inference mode , to avoid gradient calculation for the model variables\n",
        "def optimize(model,device,lr=1e3,momentum=0.9,iterations=300,step_size=40,gamma=0.1):\n",
        "\n",
        "    x=torch.randn((1,3,256,256))*0.1 # since we want to optimize input not model\n",
        "    print(x.is_leaf)\n",
        "    img=cv.imread('/content/pic10.jpg') #read as BGR\n",
        "    img=np.float32(cv.cvtColor(img, cv.COLOR_BGR2RGB)) #my output is in rgb mode\n",
        "    img/=255\n",
        "    torch_img=transform(img)\n",
        "    img=torch_img.unsqueeze(0) # adding an extra dimension as axis=0.\n",
        "\n",
        "    model=model.to(device)\n",
        "    img=img.to(device)\n",
        "    x=x.to(device)\n",
        "    x.requires_grad=True\n",
        "    print(x.is_leaf)\n",
        "    lr=lr\n",
        "    momentum=momentum\n",
        "\n",
        "    model.eval()\n",
        "    #model.requires_grad(False)\n",
        "\n",
        "    _,embed_out=model(img)\n",
        "\n",
        "    embed=embed_out[0]\n",
        "\n",
        "    total_loss=0.0\n",
        "    iterations=iterations\n",
        "\n",
        "    optimizer=torch.optim.SGD([x],lr = lr, momentum=momentum)\n",
        "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = step_size, gamma = gamma)\n",
        "\n",
        "    for i in range(iterations):\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      _,hookout=model(x)\n",
        "      out=hookout[0]\n",
        "\n",
        "      l2norm=l2_loss(out,embed)*1e-1\n",
        "      tvbregularise=variation_loss(x)*1e-5\n",
        "      #print(l2norm,tvbregularise)\n",
        "      total_loss=l2norm+tvbregularise\n",
        "\n",
        "      total_loss.backward(retain_graph=True)\n",
        "\n",
        "      optimizer.step()\n",
        "      #scheduler.step()\n",
        "\n",
        "      #total_loss.backward(retain_graph = True)\n",
        "\n",
        "      if i%20==19:\n",
        "        print(i,total_loss)\n",
        "        recreate_image(x)\n",
        "\n",
        "    return x#.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "6dGfWaGYUZXv"
      },
      "outputs": [],
      "source": [
        "model=Net()\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saiAO0tiuOwx"
      },
      "outputs": [],
      "source": [
        "t=optimize(model,device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FC6vHx6w0--f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}