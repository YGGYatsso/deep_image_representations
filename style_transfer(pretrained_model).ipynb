{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0MLdnUNvyQ7V"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as tf\n",
        "from PIL import Image\n",
        "from typing import Optional\n",
        "from torch.nn.modules.instancenorm import InstanceNorm2d\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "#from torchsummary import  summary\n",
        "#!pip install torchviz\n",
        "#from torchviz import make_dot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#functions and model structure is used from another repo"
      ],
      "metadata": {
        "id": "rEueUH_3R27z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def per_channel_normalize(x):\n",
        "    if x.ndim == 3:\n",
        "        mu = torch.mean(x, dim=[1, 2])\n",
        "        std = torch.std(x, dim=[1, 2])\n",
        "        return 0.5 + (x - mu[:, None, None]) / (2 * std[:, None, None])\n",
        "    elif x.ndim == 4:\n",
        "        mu = torch.mean(x, dim=[2, 3])\n",
        "        std = torch.std(x, dim=[2, 3])\n",
        "        return 0.5 + (x - mu[:, :, None, None]) / (2 * std[:, :, None, None])\n",
        "    raise Exception(\"not implemented\")"
      ],
      "metadata": {
        "id": "MPVeKJgz1_Ut"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pillow_transform(image_size: Optional[int]):\n",
        "    if image_size is None:\n",
        "        transforms = []\n",
        "    else:\n",
        "        transforms = [\n",
        "            tf.Resize(image_size),\n",
        "            tf.CenterCrop(image_size),\n",
        "        ]\n",
        "    transforms.append(tf.ToTensor())\n",
        "    return tf.Compose(transforms)"
      ],
      "metadata": {
        "id": "o-NQk0yr4KqD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(file_name: Optional[str], image=None) -> torch.Tensor:\n",
        "    \"\"\"Load an image so that its shape is (B, C, H, W) and it's normalized to\n",
        "    the range [0, 1].\n",
        "    \"\"\"\n",
        "    transform = get_pillow_transform(None)\n",
        "    if file_name is not None:\n",
        "        assert image is None\n",
        "        image = Image.open(file_name)\n",
        "    return transform(image).unsqueeze(0)\n"
      ],
      "metadata": {
        "id": "pQpiunAG5hf2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CVVNIkzs1MKc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"class residualBlock(nn.Module):\n",
        "  def __init__(self,inchannels):\n",
        "    super(residualBlock,self).__init__()\n",
        "    self.conv1=nn.Conv2d(inchannels,inchannels,3,1,1)\n",
        "    self.conv2=nn bv 56\n",
        "\"\"\"\n",
        "\"\"\"class style_net(nn.Module):\n",
        "  def __init__(self,norm=True):\n",
        "    super(style_net,self).__init__()\n",
        "    self.downsample= nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 9, 1,4),nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, 2,1),nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, 2,1),nn.ReLU(),\n",
        "        )\n",
        "    self.residual=nn.Sequential(*[ residualBlock(128) for i in range(5)])\n",
        "\n",
        "    self.up=nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),nn.Conv2d(128, 64, 3, 1,1),\n",
        "            nn.Upsample(scale_factor=2),nn.Conv2d(64, 32, 3, 1,1)\n",
        "        )\n",
        "    self.conv=nn.Conv2d(32,3,9,1,4)\n",
        "    self.img_norm=norm\n",
        "  def forward(self,x):\n",
        "    if self.img_norm==True:\n",
        "      x=per_channel_normalize(x)\n",
        "    y1=self.downsample(x)\n",
        "    y2=self.residual(y1)\n",
        "    y3=self.up(y2)\n",
        "    y=self.conv(y3)\n",
        "\n",
        "    return torch.tanh(y) * 0.5 + 0.5\"\"\""
      ],
      "metadata": {
        "id": "R1OsxCxL8uXm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "357d4554-33b0-4bdf-db23-130de4357cd1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'class style_net(nn.Module):\\n  def __init__(self,norm=True):\\n    super(style_net,self).__init__()\\n    self.downsample= nn.Sequential(\\n            nn.Conv2d(3, 32, 9, 1,4),nn.ReLU(),\\n            nn.Conv2d(32, 64, 3, 2,1),nn.ReLU(),\\n            nn.Conv2d(64, 128, 3, 2,1),nn.ReLU(),\\n        )\\n    self.residual=nn.Sequential(*[ residualBlock(128) for i in range(5)])\\n\\n    self.up=nn.Sequential(\\n            nn.Upsample(scale_factor=2),nn.Conv2d(128, 64, 3, 1,1),\\n            nn.Upsample(scale_factor=2),nn.Conv2d(64, 32, 3, 1,1)\\n        )\\n    self.conv=nn.Conv2d(32,3,9,1,4)\\n    self.img_norm=norm\\n  def forward(self,x):\\n    if self.img_norm==True:\\n      x=per_channel_normalize(x)\\n    y1=self.downsample(x)\\n    y2=self.residual(y1)\\n    y3=self.up(y2)\\n    y=self.conv(y3)\\n\\n    return torch.tanh(y) * 0.5 + 0.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TKiZ762JNFDS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionBlock(nn.Module):\n",
        "    \"\"\"This combines convolution with instance normalization.\"\"\"\n",
        "\n",
        "    reflection_pad: nn.ReflectionPad2d\n",
        "    conv: nn.Conv2d\n",
        "    instance_norm: InstanceNorm2d\n",
        "    no_norm: bool\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, no_norm=False):\n",
        "        super(ConvolutionBlock, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "        self.instance_norm = nn.InstanceNorm2d(out_channels, affine=True)\n",
        "        self.no_norm = no_norm\n",
        "\n",
        "    def forward(self, image: torch.Tensor):\n",
        "        pad = self.reflection_pad(image)\n",
        "        conv = self.conv(pad)\n",
        "        if self.no_norm:\n",
        "            return conv\n",
        "        return self.instance_norm(conv)"
      ],
      "metadata": {
        "id": "hqXPqLkoQsgw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"This is a residual block as defined in \"Perceptual Losses for Real-Time\n",
        "    Style Transfer and Super-Resolution: Supplementary Material\" by Johnson et\n",
        "    al. See https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    conv1: ConvolutionBlock\n",
        "    relu: nn.ReLU\n",
        "    conv2: ConvolutionBlock\n",
        "\n",
        "    def __init__(self, num_channels: int) -> None:\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = ConvolutionBlock(\n",
        "            num_channels, num_channels, kernel_size=3, stride=1\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = ConvolutionBlock(\n",
        "            num_channels, num_channels, kernel_size=3, stride=1\n",
        "        )\n",
        "\n",
        "    def forward(self, image: torch.Tensor):\n",
        "        return self.conv2(self.relu(self.conv1(image))) + image"
      ],
      "metadata": {
        "id": "vWgCMZhXQ08w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpsampleBlock(nn.Module):\n",
        "    \"\"\"This increases resolution by upsampling and then convolving.\"\"\"\n",
        "\n",
        "    upsample: nn.Upsample\n",
        "    conv: ConvolutionBlock\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        scale_factor: float,\n",
        "    ) -> None:\n",
        "\n",
        "        super(UpsampleBlock, self).__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=scale_factor)\n",
        "        self.conv = ConvolutionBlock(in_channels, out_channels, kernel_size, 1)\n",
        "\n",
        "    def forward(self, image: torch.Tensor):\n",
        "        return self.conv(self.upsample(image))"
      ],
      "metadata": {
        "id": "Jeq0d_TlQ-f3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class StylizationModel(nn.Module):\n",
        "    \"\"\"This is the stylization network described here:\n",
        "    https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalize) -> None:\n",
        "        super(StylizationModel, self).__init__()\n",
        "\n",
        "        self.down_convolution = nn.Sequential(\n",
        "            ConvolutionBlock(3, 32, kernel_size=9, stride=1),\n",
        "            nn.ReLU(),\n",
        "            ConvolutionBlock(32, 64, kernel_size=3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            ConvolutionBlock(64, 128, kernel_size=3, stride=2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.residual = nn.Sequential(*[ResidualBlock(128) for _ in range(5)])\n",
        "        self.up_convolution = nn.Sequential(\n",
        "            UpsampleBlock(128, 64, 3, 2),\n",
        "            nn.ReLU(),\n",
        "            UpsampleBlock(64, 32, 3, 2),\n",
        "            nn.ReLU(),\n",
        "            ConvolutionBlock(32, 3, 9, 1),\n",
        "        )\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def forward(self, image: torch.Tensor):\n",
        "        if self.normalize:\n",
        "            image = per_channel_normalize(image)\n",
        "        x = self.down_convolution(image)\n",
        "        x = self.residual(x)\n",
        "        x = self.up_convolution(x)\n",
        "        return torch.tanh(x) * 0.5 + 0.5\n"
      ],
      "metadata": {
        "id": "SufEhyR8RBUp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize = tf.Compose(\n",
        "    [tf.Resize((256, 256)),]\n",
        ")\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "EsDhWh6t5ST8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "sn4aag1z4lOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4092827-94fc-4223-895c-acf55e697d4a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def processimg(image):\n",
        "  image_3=np.squeeze(image,axis=0) #removing dimension\n",
        "  image=np.transpose(image_3,(1,2,0)) # (c,h,w)->(h,w,c)\n",
        "  image=np.clip(image,0,1)\n",
        "  con_image=np.uint8(image*255.0)\n",
        "  con_image=cv.cvtColor(con_image, cv.COLOR_RGB2BGR) # rgb->bgr , since cv2_imshow assumes image is in bgr format\n",
        "  return con_image"
      ],
      "metadata": {
        "id": "xzpIu0FIxmDJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perimage(content_img_path,style_img_path,model_output):\n",
        "    #target image(content)\n",
        "    img=cv.imread(content_img_path)\n",
        "    img=cv.resize(img,(256,256))\n",
        "    #source image(style)\n",
        "    style=cv.imread(style_img_path)\n",
        "    style=cv.resize(style,(256,256))\n",
        "    #style applied image\n",
        "    mix_img=processimg(model_output)\n",
        "    #creating a stack of image : imagec:style:style_applied_output\n",
        "    stack=np.zeros((256,768,3))\n",
        "    stack[:,:256,:]=img\n",
        "    stack[:,256:512,:]=style\n",
        "    stack[:,512:,:]=mix_img\n",
        "\n",
        "    return stack"
      ],
      "metadata": {
        "id": "gU3FPuIWxm4O"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test(contentimg_path_list,styleimg_path_list,pretrained_wets_pathlist):\n",
        "    #loading model and pretrained weights\n",
        "\n",
        "    assert len(styleimg_path_list)==len(pretrained_wets_pathlist), \"for each style image , there must be one pretrained weights\"\n",
        "    model = StylizationModel(True)\n",
        "    model.eval()\n",
        "\n",
        "    #model.load_state_dict(torch.load(pretrained_weights_pth))\n",
        "    #model = model.to(device)\n",
        "\n",
        "    output_stacks=[] # len(contentimg_path_list)*len(styleimg_path_list)\n",
        "    for i,weights_path in enumerate(pretrained_wets_pathlist):\n",
        "      style_img_path=styleimg_path_list[i]\n",
        "\n",
        "      model.load_state_dict(torch.load(weights_path))\n",
        "      model = model.to(device)\n",
        "\n",
        "      for j in contentimg_path_list:\n",
        "\n",
        "        content_img_path=j\n",
        "        print(content_img_path)\n",
        "        #loading image and applying stylisation to it.\n",
        "        img = load_image(content_img_path)\n",
        "        image = resize(img)\n",
        "        output_test = model(image.to(device)).cpu().detach().numpy()\n",
        "\n",
        "        # now we have output and other images path list\n",
        "\n",
        "        get=perimage(content_img_path,style_img_path,output_test)\n",
        "\n",
        "        output_stacks.append(get)\n",
        "\n",
        "    return output_stacks"
      ],
      "metadata": {
        "id": "oy0jwnrEYIL9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_wets_pathlist=['/content/haring_model_0_09000.pth',\n",
        "                           '/content/stairs_final_model.pth'\n",
        "                       ]\n",
        "\n",
        "styleimg_path_list=[ '/content/haring.jpg',\n",
        "                      '/content/stairs.jpg'\n",
        "                    ]\n",
        "\n",
        "contentimg_path_list=[ '/content/pic10.jpg',\n",
        "                   '/content/pic11.jpg',\n",
        "                       '/content/pic5.jpg'\n",
        "                      ]"
      ],
      "metadata": {
        "id": "gz7zg_qIY85H"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stack_img=test(contentimg_path_list,styleimg_path_list,pretrained_wets_pathlist)"
      ],
      "metadata": {
        "id": "FDDczsS89xGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in enumerate(stack_img):\n",
        "  cv.imwrite(str(i+6)+\".jpg\",j)"
      ],
      "metadata": {
        "id": "xJ_i7T9XT6J0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(stack_img[0])"
      ],
      "metadata": {
        "id": "rzQPQoFp2i_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#so yep we got it that the issue was parameters keys in my custom model were not matching with the one used while training .\n",
        "#after using original model(basically original parameter keys it worked fine)."
      ],
      "metadata": {
        "id": "QwWaz8ddVyBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#but when i am  displaying the images i am not getting pleasing output\n",
        "#does pytorch or opencv uses different method to adjust , the color irradiance .\n",
        "#now color issue is also resolved , PIL was reading image as RGB , but opencv_inshow displays image as bgr ,so need to convert color space before showing the image"
      ],
      "metadata": {
        "id": "s18gVUuLVyFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bhODrt06VyIg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}